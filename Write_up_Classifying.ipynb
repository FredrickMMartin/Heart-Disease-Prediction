{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea065bc9",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1edb08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-20T11:21:42.625794Z",
     "start_time": "2021-11-20T11:21:42.613466Z"
    },
    "hidden": true
   },
   "source": [
    "To create an algorithm to predict the presence of heart disease based on the values of 13 features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aa3931",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Import libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a24fed9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T08:13:28.347963Z",
     "start_time": "2022-06-09T08:13:28.343462Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# general libraries\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "\n",
    "# pipeline functions\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# useful additional functions\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df539294",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T07:33:28.489088Z",
     "start_time": "2022-06-09T07:33:27.038135Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import learning_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec579bd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T07:33:28.534774Z",
     "start_time": "2022-06-09T07:33:28.491515Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# read pickled df \n",
    "df = pd.read_pickle('extended_heart_disease.pkl')\n",
    "\n",
    "\n",
    "# split data\n",
    "features = list(df.columns)\n",
    "features.remove('target')\n",
    "\n",
    "#split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[features]\n",
    "                                                    , df['target']\n",
    "                                                    , test_size = 0.2\n",
    "                                                    , random_state = 42\n",
    "                                                    , stratify = df['target'])\n",
    "\n",
    "# read valuable lists\n",
    "with open('useful_lists.pkl', 'rb') as f:\n",
    "    valuable_lists = pkl.load(f)\n",
    "\n",
    "num_features = valuable_lists['num_features']\n",
    "cat_features = valuable_lists['cat_features']\n",
    "products = valuable_lists['products']\n",
    "divisions = valuable_lists['divisions']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbee9fef",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Establishing baseline performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b052f6",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Before we begin trying to develop an optimised classifier we should establish a base line to compare improvement against.\n",
    "We will be using f1-scoring to control for any effect of the higher proportion of negatives in our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51d75268",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T08:13:37.679423Z",
     "start_time": "2022-05-18T08:13:35.080781Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_min_samples_leaf</th>\n",
       "      <th>param_random_state</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.133639</td>\n",
       "      <td>0.023049</td>\n",
       "      <td>0.013601</td>\n",
       "      <td>0.003199</td>\n",
       "      <td>0.01</td>\n",
       "      <td>42</td>\n",
       "      <td>{'min_samples_leaf': 0.01, 'random_state': 42}</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.852459</td>\n",
       "      <td>0.830189</td>\n",
       "      <td>0.897959</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.842495</td>\n",
       "      <td>0.036234</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.098271</td>\n",
       "      <td>0.017626</td>\n",
       "      <td>0.010782</td>\n",
       "      <td>0.001382</td>\n",
       "      <td>0.05</td>\n",
       "      <td>42</td>\n",
       "      <td>{'min_samples_leaf': 0.05, 'random_state': 42}</td>\n",
       "      <td>0.813559</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.856038</td>\n",
       "      <td>0.028780</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.114359</td>\n",
       "      <td>0.021655</td>\n",
       "      <td>0.012602</td>\n",
       "      <td>0.002598</td>\n",
       "      <td>0.1</td>\n",
       "      <td>42</td>\n",
       "      <td>{'min_samples_leaf': 0.1, 'random_state': 42}</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.852459</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.845772</td>\n",
       "      <td>0.028704</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.096977</td>\n",
       "      <td>0.013552</td>\n",
       "      <td>0.011010</td>\n",
       "      <td>0.002994</td>\n",
       "      <td>0.2</td>\n",
       "      <td>42</td>\n",
       "      <td>{'min_samples_leaf': 0.2, 'random_state': 42}</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.852459</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.857061</td>\n",
       "      <td>0.024596</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       0.133639      0.023049         0.013601        0.003199   \n",
       "1       0.098271      0.017626         0.010782        0.001382   \n",
       "2       0.114359      0.021655         0.012602        0.002598   \n",
       "3       0.096977      0.013552         0.011010        0.002994   \n",
       "\n",
       "  param_min_samples_leaf param_random_state  \\\n",
       "0                   0.01                 42   \n",
       "1                   0.05                 42   \n",
       "2                    0.1                 42   \n",
       "3                    0.2                 42   \n",
       "\n",
       "                                           params  split0_test_score  \\\n",
       "0  {'min_samples_leaf': 0.01, 'random_state': 42}           0.785714   \n",
       "1  {'min_samples_leaf': 0.05, 'random_state': 42}           0.813559   \n",
       "2   {'min_samples_leaf': 0.1, 'random_state': 42}           0.793103   \n",
       "3   {'min_samples_leaf': 0.2, 'random_state': 42}           0.827586   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0           0.852459           0.830189           0.897959           0.846154   \n",
       "1           0.866667           0.846154           0.901961           0.851852   \n",
       "2           0.852459           0.846154           0.880000           0.857143   \n",
       "3           0.852459           0.846154           0.901961           0.857143   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0         0.842495        0.036234                4  \n",
       "1         0.856038        0.028780                2  \n",
       "2         0.845772        0.028704                3  \n",
       "3         0.857061        0.024596                1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'min_samples_leaf' : (0.01, 0.05, 0.1, 0.2), 'random_state' : [42]}\n",
    "\n",
    "# the baseline should be taken on the original features, before any additions were made\n",
    "original_features = ['thal'] + num_features + cat_features \n",
    "\n",
    "gscv = GridSearchCV(RandomForestClassifier(), parameters, cv=5, scoring = 'f1')\n",
    "gscv.fit(X_train[original_features], y_train)\n",
    "\n",
    "pd.DataFrame(gscv.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca5e286",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Ok so based on a very simple Grid Search we can take a baseline of 0.857"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b8f287",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Pipeline design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e5e616",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Qualities that we require from the pipeline:\n",
    "- categorical features must be encoded to ensure that they are not treated numerically\n",
    "- numerical features, where possible, should be scaled\n",
    "- we should have flexibility in using different algorithms/feature subsets within the same grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5058960",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T07:33:40.129669Z",
     "start_time": "2022-06-09T07:33:40.122729Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# for transforming numerical features\n",
    "numeric_transformer = StandardScaler()\n",
    "\n",
    "# for transforming categorical features\n",
    "# We have encoded categorical features so that, e.g. 1 and 2 are not considered more similar than 1 and 3 when all 3 values just encode different categories.\n",
    "categorical_transformer = OneHotEncoder(handle_unknown = 'ignore')\n",
    "\n",
    "# create ColumnTransformer to transform categorical and numerical features separately\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers =[\n",
    "        ('num', numeric_transformer, selector(dtype_exclude=\"category\")),\n",
    "        ('cat', categorical_transformer, selector(dtype_include=\"category\"))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87b2bf07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T07:33:42.214847Z",
     "start_time": "2022-06-09T07:33:42.204621Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# this enables us to try different classifiers within the same grid search\n",
    "# taken from the user cgnorthcutt on stackoverflow\n",
    "class ClfSwitcher(BaseEstimator):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        estimator = RandomForestClassifier(),\n",
    "        ):\n",
    "        \"\"\"\n",
    "        A Custom BaseEstimator that can switch between classifiers.\n",
    "        :param estimator: sklearn object - The classifier\n",
    "        \"\"\" \n",
    "\n",
    "        self.estimator = estimator\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None, **kwargs):\n",
    "        self.estimator.fit(X, y)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        return self.estimator.predict(X)\n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.estimator.predict_proba(X)\n",
    "\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return self.estimator.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33d4ae44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T07:33:45.475311Z",
     "start_time": "2022-06-09T07:33:45.461564Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# this class is designed to enable switching between the features that are used within the same grid search\n",
    "class FeatureSelector(BaseEstimator):\n",
    "\n",
    "    def __init__(self, chosen_features = None):\n",
    "        self.chosen_features = chosen_features\n",
    "\n",
    "    def fit(self, X, y=None, **kwargs):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X[self.chosen_features]\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dd27cb",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Exploring performance on feature subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec18978",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f39548a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T07:34:02.123415Z",
     "start_time": "2022-06-09T07:34:02.119874Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create different subsets of features to test performance on\n",
    "original_features =  ['thal'] + num_features + cat_features\n",
    "\n",
    "feature_subsets = (\n",
    "    original_features\n",
    "    , original_features + divisions\n",
    "    , original_features + products\n",
    "    , original_features + divisions + products\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f404ff",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Initial gridsearch for classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f226556",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's begin by just taking a look at 3 simple classifiers and see how they perform when given a small scope of variation in their hyper-parameters.\n",
    "We will use grid-search to explore different instances of each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8bca425",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T08:13:37.757742Z",
     "start_time": "2022-05-18T08:13:37.745968Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('selector', FeatureSelector())\n",
    "    , ('preprocessor', preprocessor)\n",
    "    , ('clf', ClfSwitcher())\n",
    "])\n",
    "\n",
    "parameters = [{\n",
    "        'clf__estimator' : [RandomForestClassifier(random_state = 42)]\n",
    "        , 'clf__estimator__class_weight' : ('balanced', None)\n",
    "        , 'selector__chosen_features' : feature_subsets\n",
    "        , 'clf__estimator__min_samples_leaf' : (0.02, 0.05, 0.1, 0.2, 0.5)\n",
    "    }\n",
    "    \n",
    "    ,{\n",
    "       'clf__estimator' : [LogisticRegression(random_state = 42)]\n",
    "        , 'clf__estimator__class_weight' : ('balanced', None)\n",
    "        , 'selector__chosen_features' : feature_subsets\n",
    "        , 'clf__estimator__C' : (1.0, 0.3, 0.1, 0.03, 0.01)\n",
    "    }\n",
    "    \n",
    "    ,{\n",
    "       'clf__estimator' : [SVC(random_state = 42)]\n",
    "        , 'clf__estimator__class_weight' : ('balanced', None)\n",
    "        , 'selector__chosen_features' : feature_subsets\n",
    "        , 'clf__estimator__C' : (1.0, 0.3, 0.1, 0.03, 0.01)\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb05c69d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here we are varying a number of different parameters.\n",
    "Firstly, we are testing with different algorithms.  \n",
    "Secondly, we are testing with balancing the weights in our cost function, in cases where class_weight = 'balanced', the cost function will take into account the fact that there are slightly more cases where heart disease is present than cases where it is not.\n",
    "Finally, we are testing different subsets of the features that we have brought through from the expanded feature set. This will allow us to measure the impact of including features and help us establish whether to include them within our algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a4e9e1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T08:14:14.898070Z",
     "start_time": "2022-05-18T08:13:37.759412Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('selector', FeatureSelector()),\n",
       "                                       ('preprocessor',\n",
       "                                        ColumnTransformer(transformers=[('num',\n",
       "                                                                         StandardScaler(),\n",
       "                                                                         <sklearn.compose._column_transformer.make_column_selector object at 0x00000291E2D5EF70>),\n",
       "                                                                        ('cat',\n",
       "                                                                         OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                                         <sklearn.compose._column_transformer.make_column_selector object at 0x0000...\n",
       "                                                         'trestbps', 'chol',\n",
       "                                                         'thalach', 'oldpeak',\n",
       "                                                         'sex', 'cp', 'fbs',\n",
       "                                                         'restecg', 'exang',\n",
       "                                                         'slope', 'ca',\n",
       "                                                         'age / trestbps',\n",
       "                                                         'age / chol',\n",
       "                                                         'age / thalach',\n",
       "                                                         'trestbps / age',\n",
       "                                                         'trestbps / chol',\n",
       "                                                         'trestbps / thalach',\n",
       "                                                         'chol / age',\n",
       "                                                         'chol / trestbps',\n",
       "                                                         'chol / thalach',\n",
       "                                                         'thalach / age',\n",
       "                                                         'thalach / trestbps',\n",
       "                                                         'thalach / chol',\n",
       "                                                         'oldpeak / age',\n",
       "                                                         'oldpeak / trestbps',\n",
       "                                                         'oldpeak / chol',\n",
       "                                                         'oldpeak / thalach',\n",
       "                                                         'age * age', ...])}],\n",
       "             scoring='f1')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gscv = GridSearchCV(pipeline, parameters, cv = 5, scoring = 'f1')\n",
    "gscv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584adac6",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Exploring results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d2d1d3",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Presenting results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3c934f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In order to perform analysis on the results of the grid search it is easier for us to format the results as a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cefe7af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T08:14:14.913538Z",
     "start_time": "2022-05-18T08:14:14.900456Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gs1_results = pd.DataFrame(gscv.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "883af146",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T08:14:14.929569Z",
     "start_time": "2022-05-18T08:14:14.915445Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def renaming_features(x):\n",
    "    '''\n",
    "    The purpose of this function is to receive one of 4 specified features sets (as a list) and to return a string, naming that feature set\n",
    "    '''\n",
    "    if x == original_features:\n",
    "        return 'original features'\n",
    "    elif x == original_features + divisions:\n",
    "        return 'original features + divisions'\n",
    "    elif x == original_features + products:\n",
    "        return 'original features + products'\n",
    "    elif x == original_features + divisions + products:\n",
    "        return 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb97b150",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T08:14:14.945094Z",
     "start_time": "2022-05-18T08:14:14.932081Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# add a column for the feature subset name\n",
    "gs1_results['features'] = gs1_results['param_selector__chosen_features'].apply(lambda x: renaming_features(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87ad4c62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T08:14:14.960203Z",
     "start_time": "2022-05-18T08:14:14.947404Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# separate out instances of different algorithms\n",
    "rf1_results = gs1_results.loc[0 : 39].copy()\n",
    "logres1_results = gs1_results.loc[40 : 79].copy()\n",
    "svm1_results = gs1_results.loc[80 : 119].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaa32c6",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Top performers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a9bd2d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First of all let us look at the top performing of all the classifier instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b179dd5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T08:14:14.990895Z",
     "start_time": "2022-05-18T08:14:14.961753Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_clf__estimator</th>\n",
       "      <th>features</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>LogisticRegression(C=0.3, random_state=42)</td>\n",
       "      <td>original features + divisions</td>\n",
       "      <td>0.862649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>LogisticRegression(C=0.3, random_state=42)</td>\n",
       "      <td>original features + divisions</td>\n",
       "      <td>0.862327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>LogisticRegression(C=0.3, random_state=42)</td>\n",
       "      <td>original features</td>\n",
       "      <td>0.861140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>LogisticRegression(C=0.3, random_state=42)</td>\n",
       "      <td>original features + products</td>\n",
       "      <td>0.861140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>LogisticRegression(C=0.3, random_state=42)</td>\n",
       "      <td>original features + divisions</td>\n",
       "      <td>0.859593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>LogisticRegression(C=0.3, random_state=42)</td>\n",
       "      <td>original features</td>\n",
       "      <td>0.858844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>RandomForestClassifier(random_state=42)</td>\n",
       "      <td>original features</td>\n",
       "      <td>0.858572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>LogisticRegression(C=0.3, random_state=42)</td>\n",
       "      <td>all</td>\n",
       "      <td>0.858205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>LogisticRegression(C=0.3, random_state=42)</td>\n",
       "      <td>original features</td>\n",
       "      <td>0.857455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>RandomForestClassifier(random_state=42)</td>\n",
       "      <td>original features</td>\n",
       "      <td>0.857179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          param_clf__estimator                       features  \\\n",
       "53  LogisticRegression(C=0.3, random_state=42)  original features + divisions   \n",
       "49  LogisticRegression(C=0.3, random_state=42)  original features + divisions   \n",
       "52  LogisticRegression(C=0.3, random_state=42)              original features   \n",
       "54  LogisticRegression(C=0.3, random_state=42)   original features + products   \n",
       "61  LogisticRegression(C=0.3, random_state=42)  original features + divisions   \n",
       "40  LogisticRegression(C=0.3, random_state=42)              original features   \n",
       "28     RandomForestClassifier(random_state=42)              original features   \n",
       "55  LogisticRegression(C=0.3, random_state=42)                            all   \n",
       "48  LogisticRegression(C=0.3, random_state=42)              original features   \n",
       "24     RandomForestClassifier(random_state=42)              original features   \n",
       "\n",
       "    mean_test_score  \n",
       "53         0.862649  \n",
       "49         0.862327  \n",
       "52         0.861140  \n",
       "54         0.861140  \n",
       "61         0.859593  \n",
       "40         0.858844  \n",
       "28         0.858572  \n",
       "55         0.858205  \n",
       "48         0.857455  \n",
       "24         0.857179  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs1_results.sort_values('mean_test_score', inplace = True, ascending = False)\n",
    "gs1_results.head(10)[['param_clf__estimator', 'features', 'mean_test_score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b43d9c7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So whilst the top 10 is predominantly composed of LogisticRegression algorithms, the top 2 are both RandomForest.\n",
    "Performance is not noteworthily better than baseline performance, particularly given the number of instances that we are testing here.\n",
    "120 instances were tested and the performance of the 10th best is only just above that of the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c13acd7",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eaa270cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T08:14:15.006036Z",
     "start_time": "2022-05-18T08:14:14.992442Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>param_clf__estimator__class_weight</th>\n",
       "      <th>param_clf__estimator__min_samples_leaf</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>original features</td>\n",
       "      <td>None</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.858572</td>\n",
       "      <td>0.057828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>original features</td>\n",
       "      <td>None</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.857179</td>\n",
       "      <td>0.030151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>original features</td>\n",
       "      <td>None</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.855353</td>\n",
       "      <td>0.051857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>original features</td>\n",
       "      <td>None</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.846730</td>\n",
       "      <td>0.023988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>original features</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.844993</td>\n",
       "      <td>0.053216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>original features</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.834522</td>\n",
       "      <td>0.046426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>original features + products</td>\n",
       "      <td>None</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.826956</td>\n",
       "      <td>0.031229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>original features</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.826784</td>\n",
       "      <td>0.040586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>original features + divisions</td>\n",
       "      <td>None</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.825236</td>\n",
       "      <td>0.049661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>original features + products</td>\n",
       "      <td>None</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.823015</td>\n",
       "      <td>0.033650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         features param_clf__estimator__class_weight  \\\n",
       "28              original features                               None   \n",
       "24              original features                               None   \n",
       "32              original features                               None   \n",
       "20              original features                               None   \n",
       "12              original features                           balanced   \n",
       "8               original features                           balanced   \n",
       "30   original features + products                               None   \n",
       "4               original features                           balanced   \n",
       "21  original features + divisions                               None   \n",
       "26   original features + products                               None   \n",
       "\n",
       "   param_clf__estimator__min_samples_leaf  mean_test_score  std_test_score  \n",
       "28                                    0.1         0.858572        0.057828  \n",
       "24                                   0.05         0.857179        0.030151  \n",
       "32                                    0.2         0.855353        0.051857  \n",
       "20                                   0.02         0.846730        0.023988  \n",
       "12                                    0.2         0.844993        0.053216  \n",
       "8                                     0.1         0.834522        0.046426  \n",
       "30                                    0.1         0.826956        0.031229  \n",
       "4                                    0.05         0.826784        0.040586  \n",
       "21                                   0.02         0.825236        0.049661  \n",
       "26                                   0.05         0.823015        0.033650  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf1_results.sort_values('mean_test_score', inplace = True, ascending = False)\n",
    "rf_columns = ['features', 'param_clf__estimator__class_weight', 'param_clf__estimator__min_samples_leaf', 'mean_test_score', 'std_test_score']\n",
    "rf1_results.head(10)[rf_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034c4c48",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Random forest is consistently performing best when only given the original features to train on. All 8 cases where the algorithm is just trained on those features are returned in the top 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c85ff9",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cd9d3d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T08:14:15.021569Z",
     "start_time": "2022-05-18T08:14:15.007313Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>param_clf__estimator__class_weight</th>\n",
       "      <th>param_clf__estimator__C</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>original features + divisions</td>\n",
       "      <td>None</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.862649</td>\n",
       "      <td>0.041714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>original features + divisions</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.862327</td>\n",
       "      <td>0.050393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>original features</td>\n",
       "      <td>None</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.861140</td>\n",
       "      <td>0.044026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>original features + products</td>\n",
       "      <td>None</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.861140</td>\n",
       "      <td>0.044026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>original features + divisions</td>\n",
       "      <td>None</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.859593</td>\n",
       "      <td>0.043718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>original features</td>\n",
       "      <td>balanced</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.858844</td>\n",
       "      <td>0.046527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>all</td>\n",
       "      <td>None</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.858205</td>\n",
       "      <td>0.048744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>original features</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.857455</td>\n",
       "      <td>0.037908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>original features + divisions</td>\n",
       "      <td>balanced</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.855676</td>\n",
       "      <td>0.049281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>original features + products</td>\n",
       "      <td>None</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.855018</td>\n",
       "      <td>0.035780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         features param_clf__estimator__class_weight  \\\n",
       "53  original features + divisions                               None   \n",
       "49  original features + divisions                           balanced   \n",
       "52              original features                               None   \n",
       "54   original features + products                               None   \n",
       "61  original features + divisions                               None   \n",
       "40              original features                           balanced   \n",
       "55                            all                               None   \n",
       "48              original features                           balanced   \n",
       "41  original features + divisions                           balanced   \n",
       "62   original features + products                               None   \n",
       "\n",
       "   param_clf__estimator__C  mean_test_score  std_test_score  \n",
       "53                     0.3         0.862649        0.041714  \n",
       "49                     0.3         0.862327        0.050393  \n",
       "52                     0.3         0.861140        0.044026  \n",
       "54                     0.3         0.861140        0.044026  \n",
       "61                     0.1         0.859593        0.043718  \n",
       "40                     1.0         0.858844        0.046527  \n",
       "55                     0.3         0.858205        0.048744  \n",
       "48                     0.3         0.857455        0.037908  \n",
       "41                     1.0         0.855676        0.049281  \n",
       "62                     0.1         0.855018        0.035780  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logres1_results.sort_values('mean_test_score', inplace = True, ascending = False)\n",
    "logres_columns = ['features', 'param_clf__estimator__class_weight', 'param_clf__estimator__C', 'mean_test_score', 'std_test_score']\n",
    "logres1_results.head(10)[logres_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc17df5c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The range of performance within the top 10 logisitc regression options is a lot smaller than that seen with Random Forest.\n",
    "There is also a lot more variety in the features being used in each instance of the algorithm and no one set of features seems dominant.\n",
    "The most consistent feature is that 0.3 seems to be the optimal value of C."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b921ee",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0629afa6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T08:14:15.037049Z",
     "start_time": "2022-05-18T08:14:15.022893Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>param_clf__estimator__class_weight</th>\n",
       "      <th>param_clf__estimator__C</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>original features</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.840156</td>\n",
       "      <td>0.056151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>original features</td>\n",
       "      <td>balanced</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.828256</td>\n",
       "      <td>0.028627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>original features + products</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.824934</td>\n",
       "      <td>0.035799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>original features</td>\n",
       "      <td>None</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.822939</td>\n",
       "      <td>0.031733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>original features</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.822182</td>\n",
       "      <td>0.034039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>original features</td>\n",
       "      <td>balanced</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.814604</td>\n",
       "      <td>0.044908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>original features</td>\n",
       "      <td>None</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.811379</td>\n",
       "      <td>0.041196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>original features + products</td>\n",
       "      <td>balanced</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.803524</td>\n",
       "      <td>0.036047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>all</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.803038</td>\n",
       "      <td>0.048812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>all</td>\n",
       "      <td>balanced</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.801570</td>\n",
       "      <td>0.050821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         features param_clf__estimator__class_weight  \\\n",
       "84              original features                               None   \n",
       "80              original features                           balanced   \n",
       "86   original features + products                               None   \n",
       "92              original features                               None   \n",
       "88              original features                           balanced   \n",
       "96              original features                           balanced   \n",
       "100             original features                               None   \n",
       "82   original features + products                           balanced   \n",
       "87                            all                               None   \n",
       "83                            all                           balanced   \n",
       "\n",
       "    param_clf__estimator__C  mean_test_score  std_test_score  \n",
       "84                      1.0         0.840156        0.056151  \n",
       "80                      1.0         0.828256        0.028627  \n",
       "86                      1.0         0.824934        0.035799  \n",
       "92                      0.3         0.822939        0.031733  \n",
       "88                      0.3         0.822182        0.034039  \n",
       "96                      0.1         0.814604        0.044908  \n",
       "100                     0.1         0.811379        0.041196  \n",
       "82                      1.0         0.803524        0.036047  \n",
       "87                      1.0         0.803038        0.048812  \n",
       "83                      1.0         0.801570        0.050821  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm1_results.sort_values('mean_test_score', inplace = True, ascending = False)\n",
    "svm_columns = ['features', 'param_clf__estimator__class_weight', 'param_clf__estimator__C', 'mean_test_score', 'std_test_score']\n",
    "svm1_results.head(10)[svm_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9998e6",
   "metadata": {
    "hidden": true
   },
   "source": [
    "SVM is performing far below the level of random forest and logistic regression.\n",
    "The algorithm is generally performing better when trained on just the original features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44f693d",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Overarching patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a471514",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Although a small number of instances of the classifiers have outperformed the baseline, the vast majority have not.\n",
    "In fact the highest performing instance in the gridsearch was one of the 3 instances considered in the baseline.\n",
    "This indicates that nothing we have done so far has achieved any improvement on the baseline.\n",
    "\n",
    "From here on I shall not be using balanced class_weight as in most side-by-side comparisons, balancing the class weights actually reduced performance (measured on f1-score).\n",
    "\n",
    "If I were performing a more extended analysis I would explore whether this reduction is due to the fact that test_score is evaluated as a simple accuracy percentage, therefore meaning that f1-score might be better when class_weight = 'balanced', however I will not go into this here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59088ba7",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Trying to improve performance of individual algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d230d67a",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Removing scaling and one hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ff1a43",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The majority of the instances of algorithms tested above performed worse than the baseline which included no scaling/feature encoding. We will explore here whether this processing of the features is actually harming our algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3ee77b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d35861fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T08:14:24.455773Z",
     "start_time": "2022-05-18T08:14:15.038539Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all</th>\n",
       "      <th>originals</th>\n",
       "      <th>originals + divisors</th>\n",
       "      <th>originals + products</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.02</th>\n",
       "      <td>0.840616</td>\n",
       "      <td>0.813648</td>\n",
       "      <td>0.841341</td>\n",
       "      <td>0.808004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.05</th>\n",
       "      <td>0.856038</td>\n",
       "      <td>0.811671</td>\n",
       "      <td>0.831829</td>\n",
       "      <td>0.812376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.1</th>\n",
       "      <td>0.845772</td>\n",
       "      <td>0.802818</td>\n",
       "      <td>0.829333</td>\n",
       "      <td>0.811743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.2</th>\n",
       "      <td>0.857061</td>\n",
       "      <td>0.814149</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.796578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           all  originals  originals + divisors  originals + products\n",
       "0.02  0.840616   0.813648              0.841341              0.808004\n",
       "0.05  0.856038   0.811671              0.831829              0.812376\n",
       "0.1   0.845772   0.802818              0.829333              0.811743\n",
       "0.2   0.857061   0.814149              0.787879              0.796578"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('selector', FeatureSelector())\n",
    "    , ('rf', RandomForestClassifier(random_state = 42))\n",
    "])\n",
    "\n",
    "parameters = [{\n",
    "        'selector__chosen_features' : feature_subsets\n",
    "        , 'rf__min_samples_leaf' : (0.02,0.05, 0.1, 0.2)\n",
    "    }\n",
    "]\n",
    "\n",
    "gscv = GridSearchCV(pipeline, parameters, cv = 5, scoring = 'f1')\n",
    "gscv.fit(X_train, y_train)\n",
    "\n",
    "rf_no_scaling_results = pd.DataFrame(data=gscv.cv_results_['mean_test_score'].reshape(4, 4), columns=[\"all\", \"originals\", \"originals + divisors\", \"originals + products\"], index=[\"0.02\", \"0.05\", \"0.1\", \"0.2\"])\n",
    "rf_no_scaling_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e6b284",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Ok so performance here is pretty similar to that that seen above.\n",
    "this makes sense given that random forests are unaffected by feature scaling, however there could be a difference made by the OneHotEncoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778f0801",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14422c40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T08:14:27.681699Z",
     "start_time": "2022-05-18T08:14:24.458108Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\fredr\\anaconda3\\envs\\financial_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all</th>\n",
       "      <th>originals</th>\n",
       "      <th>originals + divisors</th>\n",
       "      <th>originals + products</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.853731</td>\n",
       "      <td>0.863295</td>\n",
       "      <td>0.761192</td>\n",
       "      <td>0.761280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.3</th>\n",
       "      <td>0.861138</td>\n",
       "      <td>0.860518</td>\n",
       "      <td>0.764127</td>\n",
       "      <td>0.761055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.1</th>\n",
       "      <td>0.857350</td>\n",
       "      <td>0.857511</td>\n",
       "      <td>0.765636</td>\n",
       "      <td>0.764127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.03</th>\n",
       "      <td>0.834224</td>\n",
       "      <td>0.834224</td>\n",
       "      <td>0.764243</td>\n",
       "      <td>0.756639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.01</th>\n",
       "      <td>0.786869</td>\n",
       "      <td>0.797796</td>\n",
       "      <td>0.765636</td>\n",
       "      <td>0.762779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           all  originals  originals + divisors  originals + products\n",
       "1.0   0.853731   0.863295              0.761192              0.761280\n",
       "0.3   0.861138   0.860518              0.764127              0.761055\n",
       "0.1   0.857350   0.857511              0.765636              0.764127\n",
       "0.03  0.834224   0.834224              0.764243              0.756639\n",
       "0.01  0.786869   0.797796              0.765636              0.762779"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('selector', FeatureSelector())\n",
    "    , ('logres', LogisticRegression(random_state = 42))\n",
    "])\n",
    "\n",
    "parameters = [{\n",
    "        'selector__chosen_features' : feature_subsets\n",
    "        , 'logres__C' : (1.0, 0.3, 0.1, 0.03, 0.01)\n",
    "    }\n",
    "]\n",
    "\n",
    "gscv = GridSearchCV(pipeline, parameters, cv = 5, scoring = 'f1')\n",
    "gscv.fit(X_train, y_train)\n",
    "\n",
    "logres_no_scaling_results = pd.DataFrame(data=gscv.cv_results_['mean_test_score'].reshape(5, 4), columns=[\"all\", \"originals\", \"originals + divisors\", \"originals + products\"], index=[\"1.0\", \"0.3\", \"0.1\", \"0.03\", \"0.01\"])\n",
    "logres_no_scaling_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f898c565",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As a result of no longer scaling the data we get cases where the algorithm fails to converge.\n",
    "Interestingly here, performance has dropped much more in the cases where we include the products. For cases where these are not included, the performance only changes minimally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15a1dbc",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7ef6257",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T08:14:29.245712Z",
     "start_time": "2022-05-18T08:14:27.683008Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all</th>\n",
       "      <th>originals</th>\n",
       "      <th>originals + divisors</th>\n",
       "      <th>originals + products</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.705711</td>\n",
       "      <td>0.709652</td>\n",
       "      <td>0.705832</td>\n",
       "      <td>0.705832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.3</th>\n",
       "      <td>0.705832</td>\n",
       "      <td>0.708859</td>\n",
       "      <td>0.705832</td>\n",
       "      <td>0.705832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.1</th>\n",
       "      <td>0.705832</td>\n",
       "      <td>0.705832</td>\n",
       "      <td>0.679702</td>\n",
       "      <td>0.704367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.03</th>\n",
       "      <td>0.705832</td>\n",
       "      <td>0.705832</td>\n",
       "      <td>0.705832</td>\n",
       "      <td>0.670580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.01</th>\n",
       "      <td>0.697987</td>\n",
       "      <td>0.705832</td>\n",
       "      <td>0.705832</td>\n",
       "      <td>0.705832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           all  originals  originals + divisors  originals + products\n",
       "1.0   0.705711   0.709652              0.705832              0.705832\n",
       "0.3   0.705832   0.708859              0.705832              0.705832\n",
       "0.1   0.705832   0.705832              0.679702              0.704367\n",
       "0.03  0.705832   0.705832              0.705832              0.670580\n",
       "0.01  0.697987   0.705832              0.705832              0.705832"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('selector', FeatureSelector())\n",
    "    , ('svm', SVC(random_state = 42))\n",
    "])\n",
    "\n",
    "parameters = [{\n",
    "        'selector__chosen_features' : feature_subsets\n",
    "        , 'svm__C' : (1.0, 0.3, 0.1, 0.03, 0.01)\n",
    "    }\n",
    "]\n",
    "\n",
    "gscv = GridSearchCV(pipeline, parameters, cv = 5, scoring = 'f1')\n",
    "gscv.fit(X_train, y_train)\n",
    "\n",
    "svm_no_scaling_results = pd.DataFrame(data=gscv.cv_results_['mean_test_score'].reshape(5, 4), columns=[\"all\", \"originals\", \"originals + divisors\", \"originals + products\"], index=[\"1.0\", \"0.3\", \"0.1\", \"0.03\", \"0.01\"])\n",
    "svm_no_scaling_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c049bdc3",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Performance here has dropped off significantly.\n",
    "Although SVM performance was below that of the other algorithms in the initial testing it has dropped off significantly here.\n",
    "This seems reasonable given that, by not scaling the features, the importance of features will be somewhat correlated to the variance of the values of those features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387bbfcb",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dee2420",
   "metadata": {
    "hidden": true
   },
   "source": [
    "It looks as though in the case of Random Forest, there is at most a minimal difference caused by the scaling and encoding, perhaps because of the encoding of categorical features making separation of values in these cases easier.\n",
    "As one would expect, the removing of scaling had a much more pronounced impact on the performance of the other two algorithms.\n",
    "\n",
    "Given these results, we will continue using encoding and feature scaling going forward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be4d787",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Exploring different feature subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341e9a6f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Feature Importances with Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03df2ee6",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Original features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b976277d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T08:14:29.261650Z",
     "start_time": "2022-05-18T08:14:29.246866Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def feature_importance_generator(features):\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor)\n",
    "        , ('rf', RandomForestClassifier(min_samples_leaf = 0.02))\n",
    "    ])\n",
    "    \n",
    "    '''\n",
    "    This function will allow us to generate feature importances for our instances of random forests.\n",
    "    '''\n",
    "\n",
    "    clf = pipeline.fit(X_train[features], y_train)\n",
    "    \n",
    "    # generate list of post-processing feature names\n",
    "    num_scale_features =  preprocessor.transformers_[0][2]\n",
    "    cat_one_hot_features = preprocessor.transformers_[1][1].get_feature_names(cat_features)\n",
    "    feature_names = num_scale_features + list(cat_one_hot_features)\n",
    "    \n",
    "    #collate feature importances and order\n",
    "    feature_importances = list(zip(feature_names, list(clf.steps[-1][-1].feature_importances_)))\n",
    "\n",
    "    for feature, importance in sorted(feature_importances, key=lambda x: x[1], reverse = True):\n",
    "        if importance > 0.02:\n",
    "            print(feature, \":\", importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bef87432",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-18T08:14:29.447313Z",
     "start_time": "2022-05-18T08:14:29.265877Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thal : 0.15438694098619196\n",
      "cp_0 : 0.13288572618036396\n",
      "oldpeak : 0.11369997929460755\n",
      "ca_0 : 0.09820599274494611\n",
      "thalach : 0.07280477484172251\n",
      "slope_2 : 0.06317329519240283\n",
      "chol : 0.051950614231583696\n",
      "exang_0 : 0.04929452150713346\n",
      "age : 0.042682276649829695\n",
      "exang_1 : 0.039094583264571915\n",
      "slope_1 : 0.03178104779782858\n",
      "trestbps : 0.02728349799655348\n",
      "cp_2 : 0.026476354048620893\n",
      "sex_0 : 0.02267792171633446\n"
     ]
    }
   ],
   "source": [
    "feature_importance_generator(original_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1adcec",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The results here are not as easy to interpret as we might like them to be.\n",
    "The importance of the categorical features is separated between the different encoded versions of the features and by virtue of there being more features related to (e.g. cp) than there are for numerical features, they are more likely to be selected in the random set of features that can be used to at each node and therefore be attributed more importance.\n",
    "\n",
    "Having said that, this is a good position to start from when comparing later results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bd5a65",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Generating an optimal feature set from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1f4fc8",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's trial performance when we only feed in the more powerful features and see if this helps in any way. Importantly we will see if performance is consistently improved by the addition of these features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e01cfb",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "042fb121",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T07:34:55.301184Z",
     "start_time": "2022-06-09T07:34:55.286331Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def feature_evaluater(start_features, all_features, classifier, X_train, y_train, no_improvement_limit = 1):\n",
    "    \"\"\"\n",
    "    Trains a classifier on X_train and y_train using an increasing subset of the features to identify the optimal features\n",
    "    to use from within the set.\n",
    "    Returns a df of the features added each time and the performance.\n",
    "    No_improvement_limit specifies the number of features that can be added in a row without performance improving.\n",
    "    \"\"\"\n",
    "    \n",
    "    # adjust all_features so that it now contains just the features that we want to test for inclusion\n",
    "    for feature in start_features:\n",
    "        all_features.remove(feature)\n",
    "        \n",
    "    # rename the list\n",
    "    non_included_features = all_features\n",
    "    \n",
    "    # create a new name for start_features that will be returned as the list of optimum features \n",
    "    current_features = start_features\n",
    "    \n",
    "    round_count = 1\n",
    "    \n",
    "    # create counter for number of features that have been added without improving performance\n",
    "    no_improvement_count = 0\n",
    "    \n",
    "    # set baseline for top score thus far, this variable will represent the top socre achieved thus far\n",
    "    top_score = 0.01\n",
    "    \n",
    "    # create list to keep track of scores\n",
    "    scores = []\n",
    "    \n",
    "    # create list to keep track of features added\n",
    "    features = []\n",
    "    \n",
    "    # create list to keep track of best_features\n",
    "    best_features = start_features.copy()\n",
    "\n",
    "    # loop for as long as we do not exceed the limit of added feature without improvement\n",
    "    while no_improvement_count < no_improvement_limit:\n",
    "        # create all the subsets to test\n",
    "        new_feature_subsets = tuple([current_features + [feature] for feature in all_features])\n",
    "\n",
    "        \n",
    "        pipeline = Pipeline([\n",
    "        ('selector', FeatureSelector())\n",
    "        , ('preprocessor', preprocessor)\n",
    "        , ('clf', classifier)\n",
    "        ])\n",
    "\n",
    "        parameters = {\n",
    "                'selector__chosen_features' : new_feature_subsets\n",
    "            }\n",
    "\n",
    "        gscv_99 = GridSearchCV(pipeline, parameters, cv = 5, scoring = 'f1')\n",
    "        gscv_99.fit(X_train, y_train)\n",
    "\n",
    "        gs99_results = pd.DataFrame(gscv_99.cv_results_)\n",
    "        \n",
    "        new_scores = gs99_results['mean_test_score']\n",
    "        new_feature_index = new_scores.idxmax()\n",
    "        new_features = gs99_results['param_selector__chosen_features'][new_feature_index]\n",
    "        score = new_scores[new_feature_index]\n",
    "        \n",
    "        # add the score to scores\n",
    "        scores.append(score)\n",
    "        \n",
    "        # add the new feature to features\n",
    "        features.append(new_features[-1])\n",
    "        \n",
    "        \n",
    "        current_features = new_features\n",
    "        \n",
    "        # remove the now included feature from non-included features\n",
    "        non_included_features.remove(new_features[-1])\n",
    "        round_count += 1\n",
    "\n",
    "        # test for at least minimal improvement\n",
    "        if score - top_score > 0.0005:\n",
    "            no_improvement_count = 0\n",
    "            top_score = score\n",
    "            best_features = new_features\n",
    "\n",
    "        else:\n",
    "            no_improvement_count += 1\n",
    "        \n",
    "    feature_scores = pd.DataFrame(list(zip(features, scores)),\n",
    "               columns =['Added feature', 'Score'])\n",
    "    \n",
    "    return feature_scores, best_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afbf239",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e9fb06",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Given that cp was the most important feature identified in the feature importances, let us take this as the first feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf553f13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T07:40:06.665799Z",
     "start_time": "2022-06-09T07:35:08.251161Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rf_feature_scores, rf_best_features = feature_evaluater(['cp']\n",
    "                                  , original_features + products + divisions\n",
    "                                  , RandomForestClassifier(random_state = 42, min_samples_leaf = 0.2)\n",
    "                                  , X_train\n",
    "                                  , y_train\n",
    "                                  , no_improvement_limit = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ff17b37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T07:40:06.711569Z",
     "start_time": "2022-06-09T07:40:06.668197Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Added feature</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oldpeak / age</td>\n",
       "      <td>0.821049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>thal</td>\n",
       "      <td>0.844953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ca</td>\n",
       "      <td>0.856466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>exang</td>\n",
       "      <td>0.865270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>slope</td>\n",
       "      <td>0.873985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>oldpeak / chol</td>\n",
       "      <td>0.872355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>age * thalach</td>\n",
       "      <td>0.870178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>thalach</td>\n",
       "      <td>0.875368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>trestbps * thalach</td>\n",
       "      <td>0.874736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>restecg</td>\n",
       "      <td>0.869393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>age</td>\n",
       "      <td>0.866643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Added feature     Score\n",
       "0        oldpeak / age  0.821049\n",
       "1                 thal  0.844953\n",
       "2                   ca  0.856466\n",
       "3                exang  0.865270\n",
       "4                slope  0.873985\n",
       "5       oldpeak / chol  0.872355\n",
       "6        age * thalach  0.870178\n",
       "7              thalach  0.875368\n",
       "8   trestbps * thalach  0.874736\n",
       "9              restecg  0.869393\n",
       "10                 age  0.866643"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_feature_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b2bf3d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "That is the best performance that we have seen so far from a single algorithm.\n",
    "The previous best performance that we had seen from a RF was 0.858, so performance here has increased by over 1.5%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14df84c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T07:40:06.726994Z",
     "start_time": "2022-06-09T07:40:06.716111Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cp',\n",
       " 'oldpeak / age',\n",
       " 'thal',\n",
       " 'ca',\n",
       " 'exang',\n",
       " 'slope',\n",
       " 'oldpeak / chol',\n",
       " 'age * thalach',\n",
       " 'thalach']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_best_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f330e5",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5a85f38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T07:41:28.155070Z",
     "start_time": "2022-06-09T07:40:06.730318Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "log_res_feature_scores, log_res_best_features = feature_evaluater([] ## do not give any features to begin with as we have nothing to work from \n",
    "                                              , original_features + products + divisions\n",
    "                                              , LogisticRegression(random_state = 42, C = 0.2)\n",
    "                                              , X_train\n",
    "                                              , y_train\n",
    "                                              , no_improvement_limit = 5) ##training is a lot quicker with LogRes so giving more scope for improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf487ce4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T07:41:28.185423Z",
     "start_time": "2022-06-09T07:41:28.158137Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Added feature</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thal</td>\n",
       "      <td>0.789757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>thalach / chol</td>\n",
       "      <td>0.798406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oldpeak * oldpeak</td>\n",
       "      <td>0.807593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cp</td>\n",
       "      <td>0.845465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chol * thalach</td>\n",
       "      <td>0.854392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>oldpeak / thalach</td>\n",
       "      <td>0.863319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>chol</td>\n",
       "      <td>0.863319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>restecg</td>\n",
       "      <td>0.862672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>trestbps / chol</td>\n",
       "      <td>0.863698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>chol * oldpeak</td>\n",
       "      <td>0.866865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>trestbps * oldpeak</td>\n",
       "      <td>0.863961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>chol * chol</td>\n",
       "      <td>0.863698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>chol / trestbps</td>\n",
       "      <td>0.863698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>chol / thalach</td>\n",
       "      <td>0.860965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>thalach * thalach</td>\n",
       "      <td>0.858701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Added feature     Score\n",
       "0                 thal  0.789757\n",
       "1       thalach / chol  0.798406\n",
       "2    oldpeak * oldpeak  0.807593\n",
       "3                   cp  0.845465\n",
       "4       chol * thalach  0.854392\n",
       "5    oldpeak / thalach  0.863319\n",
       "6                 chol  0.863319\n",
       "7              restecg  0.862672\n",
       "8      trestbps / chol  0.863698\n",
       "9       chol * oldpeak  0.866865\n",
       "10  trestbps * oldpeak  0.863961\n",
       "11         chol * chol  0.863698\n",
       "12     chol / trestbps  0.863698\n",
       "13      chol / thalach  0.860965\n",
       "14   thalach * thalach  0.858701"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_res_feature_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351b4eab",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is also the best performance that we have seen from Logistic Regression, going from 0.862 to 0.867."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5354095f",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b693b38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T07:42:48.657654Z",
     "start_time": "2022-06-09T07:41:28.188873Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "svm_feature_scores, svm_best_features = feature_evaluater([] ## do not give any features to begin with as we have nothing to work from\n",
    "                                      , original_features + products + divisions\n",
    "                                      , SVC(random_state = 42, C = 1.0)\n",
    "                                      , X_train\n",
    "                                      , y_train\n",
    "                                      , no_improvement_limit = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b0bc661",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T07:42:48.688300Z",
     "start_time": "2022-06-09T07:42:48.661085Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Added feature</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thal</td>\n",
       "      <td>0.796013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ca</td>\n",
       "      <td>0.810086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>exang</td>\n",
       "      <td>0.832785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thalach / chol</td>\n",
       "      <td>0.847621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>oldpeak / trestbps</td>\n",
       "      <td>0.850847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>chol / thalach</td>\n",
       "      <td>0.850343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>age * age</td>\n",
       "      <td>0.856790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>chol * chol</td>\n",
       "      <td>0.857029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sex</td>\n",
       "      <td>0.856787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>chol * thalach</td>\n",
       "      <td>0.866942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>chol</td>\n",
       "      <td>0.870727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>thalach * thalach</td>\n",
       "      <td>0.864464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>trestbps * chol</td>\n",
       "      <td>0.859486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>thalach / age</td>\n",
       "      <td>0.856673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Added feature     Score\n",
       "0                 thal  0.796013\n",
       "1                   ca  0.810086\n",
       "2                exang  0.832785\n",
       "3       thalach / chol  0.847621\n",
       "4   oldpeak / trestbps  0.850847\n",
       "5       chol / thalach  0.850343\n",
       "6            age * age  0.856790\n",
       "7          chol * chol  0.857029\n",
       "8                  sex  0.856787\n",
       "9       chol * thalach  0.866942\n",
       "10                chol  0.870727\n",
       "11   thalach * thalach  0.864464\n",
       "12     trestbps * chol  0.859486\n",
       "13       thalach / age  0.856673"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_feature_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638f489c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Performance here far outstrips that seen with any other instance of SVC and in fact that top performing feature subset here outperforms that of LogisticRegression.\n",
    "Previously the top performance seen for SVC was 0.840."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2395c6a0",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa96ea3",
   "metadata": {
    "hidden": true
   },
   "source": [
    "With more time it would be a lot better to use different random seeds to generate a more varied set of results, from which we could draw a set of features that consistently occur in the optimal feature sets.\n",
    "Confidence in these results could also have been improved by varying the hyper-parameters to test a more varied set of circumstances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0e63d0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T07:56:06.429737Z",
     "start_time": "2022-06-09T07:56:06.419551Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cp', 'oldpeak / age', 'thal', 'ca', 'exang', 'slope', 'oldpeak / chol', 'age * thalach', 'thalach'] \n",
      "\n",
      "['thal', 'thalach / chol', 'oldpeak * oldpeak', 'cp', 'chol * thalach', 'oldpeak / thalach', 'chol', 'restecg', 'trestbps / chol', 'chol * oldpeak'] \n",
      "\n",
      "['thal', 'ca', 'exang', 'thalach / chol', 'oldpeak / trestbps', 'chol / thalach', 'age * age', 'chol * chol', 'sex', 'chol * thalach', 'chol'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(rf_best_features, '\\n')\n",
    "print(log_res_best_features, '\\n')\n",
    "print(svm_best_features, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54b5d11",
   "metadata": {
    "hidden": true
   },
   "source": [
    "It is interesting to see that despite being the most important feature for the random forest algorithm, cp does not even get used by the svm model.\n",
    "Secondly, the ratios and products are more prominent for the log_res and svm models than they are in rf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c823ebc9",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1489d73b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's try to look to see how common the misclassifications are between the algorithms.\n",
    "If the overlap between errors is minimal then we can conclude that it might be possible to train an ensemble classifier to correctly classify these cases. However if all 3 algorithms are getting particular cases wrong then these issues would not be addressed by any ensemble of the classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36b5ef4",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6da00b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T07:56:14.552142Z",
     "start_time": "2022-06-09T07:56:14.538765Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def error_comparison(classifier_list, classifier_names_list, classifier_features_list, X_train, y_train):\n",
    "    \"\"\"\n",
    "    function to compare the errors of different classifiers on the same data set\n",
    "    \"\"\"\n",
    "    \n",
    "    # split the data into 5 folds\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    skf.get_n_splits(X_train, y_train)\n",
    "    \n",
    "    result = {}\n",
    "    \n",
    "    # identify the overlap in errors for each split of the data\n",
    "    for train_index, test_index in skf.split(X_train, y_train):\n",
    "        \n",
    "        # store the results for each individual data split\n",
    "        temp_results_df = pd.DataFrame()\n",
    "        \n",
    "        # identify the samples to be used in the split\n",
    "        X_train_k, X_test_k = X_train.iloc[train_index].copy(), X_train.iloc[test_index].copy()\n",
    "        y_train_k, y_test_k = y_train.iloc[train_index].copy(), y_train.iloc[test_index].copy()\n",
    "        \n",
    "        # add the correct classifications to the data frame\n",
    "        temp_results_df['y_test_k'] = y_test_k\n",
    "        \n",
    "        # find the predicted classes for each algorithm\n",
    "        for clf, clf_name, feature_list in list(zip(classifier_list, classifier_names_list, classifier_features_list)):\n",
    "             \n",
    "\n",
    "            # create the pipeline\n",
    "            pipeline = Pipeline([\n",
    "            ('selector', FeatureSelector(feature_list))\n",
    "            , ('preprocessor', preprocessor)\n",
    "            , (clf_name, clf)\n",
    "            ])\n",
    "            \n",
    "        \n",
    "            # fit pipeline\n",
    "            pipeline.fit(X_train_k[feature_list], y_train_k)\n",
    "            \n",
    "            # identify class predictions\n",
    "            predictions = pipeline.predict(X_test_k[feature_list])\n",
    "            \n",
    "            # add results to the data frame\n",
    "            temp_results_df[clf_name] = predictions\n",
    "            \n",
    "            # add prediction accuracy to dict\n",
    "            prior_results = result.get(clf_name, [])\n",
    "            prior_results.append((predictions == y_test_k).mean())\n",
    "            \n",
    "            result[clf_name] = prior_results\n",
    "        \n",
    "        # create a copy of the classifiers_name_list to remove each classifier once it has been used\n",
    "        names_copy = classifier_names_list.copy()\n",
    "        \n",
    "        # for different pairs of classifiers, identify percentage of cases when either classifier is correct \n",
    "        for clf_name_1 in classifier_names_list:\n",
    "            names_copy.remove(clf_name_1)\n",
    "            \n",
    "            for clf_name_2 in names_copy:\n",
    "            \n",
    "                new_field_name = clf_name_1 + ' or ' + clf_name_2\n",
    "                \n",
    "                #get predictions for clfs\n",
    "                predictions_1 = temp_results_df[clf_name_1]\n",
    "                predictions_2 = temp_results_df[clf_name_2]\n",
    "                \n",
    "                #add prediction accuracy to dict\n",
    "                prior_results = result.get(new_field_name, [])\n",
    "                \n",
    "                prior_results.append(((predictions_2 == y_test_k) | (predictions_1 == y_test_k)).mean())\n",
    "            \n",
    "                result[new_field_name] = prior_results\n",
    "        \n",
    "        # find percentage of cases when any classifier is correct\n",
    "        clf_name_1, clf_name_2, clf_name_3 = classifier_names_list\n",
    "        \n",
    "        predictions_1 = temp_results_df[clf_name_1]\n",
    "        predictions_2 = temp_results_df[clf_name_2]\n",
    "        predictions_3 = temp_results_df[clf_name_3]\n",
    "        \n",
    "        all_score = ((predictions_3 == y_test_k) | (predictions_2 == y_test_k) | (predictions_1 == y_test_k)).mean()\n",
    "        \n",
    "        best_of_3_score = (((predictions_2 == y_test_k) & (predictions_1 == y_test_k)) | ((predictions_3 == y_test_k) & (predictions_1 == y_test_k)) | ((predictions_3 == y_test_k) & (predictions_2 == y_test_k))).mean()\n",
    "        \n",
    "        #add 'all' prediction accuracy to dict\n",
    "        prior_results = result.get('all', [])\n",
    "                \n",
    "        prior_results.append(all_score)\n",
    "            \n",
    "        result['all'] = prior_results\n",
    "        \n",
    "        #add 'best_of_3' prediction accuracy to dict\n",
    "        prior_results = result.get('best_of_3', [])\n",
    "                \n",
    "        prior_results.append(best_of_3_score)\n",
    "            \n",
    "        result['best_of_3'] = prior_results\n",
    "             \n",
    "    final_results = pd.DataFrame(result)\n",
    "    \n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2d3ab2",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Comparing errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6181e85c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T07:56:18.604813Z",
     "start_time": "2022-06-09T07:56:17.815361Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "classifier_list = [RandomForestClassifier(random_state = 42, min_samples_leaf = 0.2)\n",
    "                   , LogisticRegression(random_state = 42, C = 0.3)\n",
    "                  , SVC(random_state = 42, C = 1.0)]\n",
    "classifier_names_list = ['rf', 'log_res', 'svm']\n",
    "classifier_features_list = [rf_best_features, log_res_best_features, svm_best_features]\n",
    "\n",
    "results_df = error_comparison(classifier_list, classifier_names_list, classifier_features_list, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "549f3e93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T07:56:23.605800Z",
     "start_time": "2022-06-09T07:56:23.583717Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf                0.847619\n",
      "log_res           0.839116\n",
      "svm               0.855442\n",
      "rf or log_res     0.897109\n",
      "rf or svm         0.909269\n",
      "log_res or svm    0.905017\n",
      "all               0.925850\n",
      "best_of_3         0.859694\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rf</th>\n",
       "      <th>log_res</th>\n",
       "      <th>svm</th>\n",
       "      <th>rf or log_res</th>\n",
       "      <th>rf or svm</th>\n",
       "      <th>log_res or svm</th>\n",
       "      <th>all</th>\n",
       "      <th>best_of_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.816327</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.877551</td>\n",
       "      <td>0.897959</td>\n",
       "      <td>0.897959</td>\n",
       "      <td>0.836735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.775510</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.897959</td>\n",
       "      <td>0.836735</td>\n",
       "      <td>0.897959</td>\n",
       "      <td>0.897959</td>\n",
       "      <td>0.897959</td>\n",
       "      <td>0.836735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.854167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.854167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         rf   log_res       svm  rf or log_res  rf or svm  log_res or svm  \\\n",
       "0  0.795918  0.816327  0.795918       0.857143   0.877551        0.897959   \n",
       "1  0.775510  0.795918  0.897959       0.836735   0.897959        0.897959   \n",
       "2  0.895833  0.791667  0.833333       0.916667   0.916667        0.895833   \n",
       "3  0.958333  0.916667  0.895833       0.958333   0.958333        0.916667   \n",
       "4  0.812500  0.875000  0.854167       0.916667   0.895833        0.916667   \n",
       "\n",
       "        all  best_of_3  \n",
       "0  0.897959   0.836735  \n",
       "1  0.897959   0.836735  \n",
       "2  0.937500   0.854167  \n",
       "3  0.958333   0.916667  \n",
       "4  0.937500   0.854167  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(results_df.mean())\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a38dff",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4a221b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The above results show the percentage of correct classification for each algorithm in different splits of the data.\n",
    "In cases where algorithms are combined (other than best of 3), if any algorithm classifies correctly then the combined algorithm is deemed to have classified correctly (this is obviously not representative of an actual ensemble algorithm).\n",
    "\n",
    "1. The first thing I note is that, on average, about 7.5% of cases per split are misclassified by every algorithm. So even if we perfected a strategy for choosing which algorithm's prediction to follow, we would still only gain a predicted 92.5% accuracy. \n",
    "2. Performance varies a great deal between splits and out of the 3 base algorithms, none performs the best in more than 2 out of 5 of the splits.\n",
    "3. If the 3 base algorithms and the 'best_out_of_3' ensemble algorithm are considered, the ensemble is always in the top 2 performers, whilst this is not true for any other algorithm. It also outperforms the base 3 algorithms on average."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a974afc",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Building an ensemble model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e007e0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Ok so we have seen that there is potential benefit in building an ensemble classifier.\n",
    "Let's begin with a simple voting classifier and look at performance there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcb04ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T08:26:49.697563Z",
     "start_time": "2022-06-09T08:26:49.679810Z"
    },
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Creating a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cbecb980",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T08:22:06.301769Z",
     "start_time": "2022-06-09T08:22:06.292158Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# build pipelines for all 3 models\n",
    "pipe_rf = Pipeline([\n",
    "    ('selector', FeatureSelector(rf_best_features))\n",
    "    , ('preprocessor', preprocessor)\n",
    "    , ('rf', RandomForestClassifier(random_state = 42, min_samples_leaf = 0.2))\n",
    "])\n",
    "\n",
    "pipe_log_res = Pipeline([\n",
    "    ('selector', FeatureSelector(log_res_best_features))\n",
    "    , ('preprocessor', preprocessor)\n",
    "    , ('log_res', LogisticRegression(random_state = 42, C = 0.3))\n",
    "])\n",
    "\n",
    "pipe_svm = Pipeline([\n",
    "    ('selector', FeatureSelector(svm_best_features))\n",
    "    , ('preprocessor', preprocessor)\n",
    "    , ('svm', SVC(random_state = 42, C = 1.0, probability = True))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3ee16743",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T08:22:08.238119Z",
     "start_time": "2022-06-09T08:22:08.220050Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create the ensemle classifier\n",
    "eclf = VotingClassifier(estimators=[('rf', pipe_rf), ('log_res', pipe_log_res), ('svm', pipe_svm)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d6cf3b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T08:25:45.158865Z",
     "start_time": "2022-06-09T08:25:40.822482Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_voting</th>\n",
       "      <th>param_weights</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.131046</td>\n",
       "      <td>0.005181</td>\n",
       "      <td>0.019754</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>hard</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'voting': 'hard'}</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.872727</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.877193</td>\n",
       "      <td>0.881266</td>\n",
       "      <td>0.021279</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.133239</td>\n",
       "      <td>0.008559</td>\n",
       "      <td>0.021331</td>\n",
       "      <td>0.003035</td>\n",
       "      <td>soft</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>{'voting': 'soft', 'weights': [1, 1, 1]}</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.847458</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.877193</td>\n",
       "      <td>0.865433</td>\n",
       "      <td>0.032867</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.126681</td>\n",
       "      <td>0.005692</td>\n",
       "      <td>0.019364</td>\n",
       "      <td>0.001449</td>\n",
       "      <td>soft</td>\n",
       "      <td>[1, 1, 0]</td>\n",
       "      <td>{'voting': 'soft', 'weights': [1, 1, 0]}</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.819672</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.896552</td>\n",
       "      <td>0.865926</td>\n",
       "      <td>0.039557</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.150650</td>\n",
       "      <td>0.006880</td>\n",
       "      <td>0.024862</td>\n",
       "      <td>0.007142</td>\n",
       "      <td>soft</td>\n",
       "      <td>[1, 0, 1]</td>\n",
       "      <td>{'voting': 'soft', 'weights': [1, 0, 1]}</td>\n",
       "      <td>0.836364</td>\n",
       "      <td>0.896552</td>\n",
       "      <td>0.836364</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.836364</td>\n",
       "      <td>0.865744</td>\n",
       "      <td>0.036948</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.165100</td>\n",
       "      <td>0.007286</td>\n",
       "      <td>0.024523</td>\n",
       "      <td>0.003863</td>\n",
       "      <td>soft</td>\n",
       "      <td>[0, 1, 1]</td>\n",
       "      <td>{'voting': 'soft', 'weights': [0, 1, 1]}</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.847458</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.857309</td>\n",
       "      <td>0.037367</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_voting  \\\n",
       "0       0.131046      0.005181         0.019754        0.000905         hard   \n",
       "1       0.133239      0.008559         0.021331        0.003035         soft   \n",
       "2       0.126681      0.005692         0.019364        0.001449         soft   \n",
       "3       0.150650      0.006880         0.024862        0.007142         soft   \n",
       "4       0.165100      0.007286         0.024523        0.003863         soft   \n",
       "\n",
       "  param_weights                                    params  split0_test_score  \\\n",
       "0           NaN                        {'voting': 'hard'}           0.866667   \n",
       "1     [1, 1, 1]  {'voting': 'soft', 'weights': [1, 1, 1]}           0.827586   \n",
       "2     [1, 1, 0]  {'voting': 'soft', 'weights': [1, 1, 0]}           0.827586   \n",
       "3     [1, 0, 1]  {'voting': 'soft', 'weights': [1, 0, 1]}           0.836364   \n",
       "4     [0, 1, 1]  {'voting': 'soft', 'weights': [0, 1, 1]}           0.807018   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0           0.866667           0.872727           0.923077           0.877193   \n",
       "1           0.847458           0.851852           0.923077           0.877193   \n",
       "2           0.819672           0.862745           0.923077           0.896552   \n",
       "3           0.896552           0.836364           0.923077           0.836364   \n",
       "4           0.847458           0.851852           0.923077           0.857143   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0         0.881266        0.021279                1  \n",
       "1         0.865433        0.032867                4  \n",
       "2         0.865926        0.039557                2  \n",
       "3         0.865744        0.036948                3  \n",
       "4         0.857309        0.037367                5  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a grid search to test different ensemble options\n",
    "parameters = [{\n",
    "        'voting' : ['hard']\n",
    "    }\n",
    "    \n",
    "    ,{\n",
    "       'voting' : ['soft']\n",
    "       , 'weights' : ([1, 1, 1], [1, 1, 0], [1, 0, 1], [0, 1, 1])\n",
    "    }]\n",
    "\n",
    "gscv = GridSearchCV(eclf, parameters, cv = 5, scoring = 'f1')\n",
    "\n",
    "gscv.fit(X_train, y_train)\n",
    "gs_results = pd.DataFrame(gscv.cv_results_)\n",
    "gs_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2956a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T08:26:17.097951Z",
     "start_time": "2022-06-09T08:26:17.093375Z"
    },
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a639f8f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The hard voting ensemble has a performance that consistently outstrips that of other models we have seen.\n",
    "Even on the splits in which it performs the worst it still outstrips the baseline average performance as well as the average performance of almost all the models that we have looked at so far."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae61485",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Evaluating final model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8127757d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T08:40:26.929109Z",
     "start_time": "2022-06-09T08:40:26.757667Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7868852459016393"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build pipelines for all 3 models\n",
    "pipe_rf = Pipeline([\n",
    "    ('selector', FeatureSelector(rf_best_features))\n",
    "    , ('preprocessor', preprocessor)\n",
    "    , ('rf', RandomForestClassifier(random_state = 42, min_samples_leaf = 0.2))\n",
    "])\n",
    "\n",
    "pipe_log_res = Pipeline([\n",
    "    ('selector', FeatureSelector(log_res_best_features))\n",
    "    , ('preprocessor', preprocessor)\n",
    "    , ('log_res', LogisticRegression(random_state = 42, C = 0.3))\n",
    "])\n",
    "\n",
    "pipe_svm = Pipeline([\n",
    "    ('selector', FeatureSelector(svm_best_features))\n",
    "    , ('preprocessor', preprocessor)\n",
    "    , ('svm', SVC(random_state = 42, C = 1.0, probability = True))\n",
    "])\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('rf', pipe_rf), ('log_res', pipe_log_res), ('svm', pipe_svm)], voting = 'hard')\n",
    "eclf.fit(X_train, y_train)\n",
    "eclf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c3dee0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is a great example of overfitting on the training data.\n",
    "\n",
    "Let's at least compare to the performance of the best baseline model to see if we have made any progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d47c0eff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T08:40:20.676330Z",
     "start_time": "2022-06-09T08:40:20.540301Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7377049180327869"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(min_samples_leaf = 0.2, random_state = 42)\n",
    "rf.fit(X_train, y_train)\n",
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ec0cde",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Ok, so in terms of beating the baseline model it looks as though we have at least done that."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
